{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "root_path = os.path.join(os.getcwd(), \"..\") # WARNING: might need to change\n",
    "sys.path.append(root_path)\n",
    "\n",
    "from src.models.conv_model import ConvModel\n",
    "\n",
    "from src.data_loaders.blogposts import BlogDataset, BlogCollatorFn\n",
    "from src.data_loaders.pan23 import PAN23Dataset, PAN23CollatorFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_params': {'conv_layers_params': [{'conv_params': {'in_channels': 32,\n",
       "     'out_channels': 128,\n",
       "     'kernel_size': 5,\n",
       "     'padding': 'same'},\n",
       "    'dim_feedforward': 128,\n",
       "    'dropout_params': {'p': 0.1}}],\n",
       "  'transformer_model': 'roberta-base',\n",
       "  'projection_head_params': {'dropout_p': 0.1,\n",
       "   'ff_dim': 256,\n",
       "   'output_dim': 128}},\n",
       " 'max_len': 512,\n",
       " 'pretrain_params': {'batch_size': 64,\n",
       "  'test_set_ratio': 0.1,\n",
       "  'steps': 20000,\n",
       "  'learning_rate': 0.0001,\n",
       "  'weight_decay': 0.01,\n",
       "  'unfrozen_layers': 2},\n",
       " 'pan_train_params': {'batch_size': 8,\n",
       "  'epochs': 50,\n",
       "  'lr': 0.0001,\n",
       "  'weight_decay': 0.01,\n",
       "  'unfrozen_layers': 2},\n",
       " 'prefix_file_name': 'conv_transformer_base',\n",
       " 'out_dir': 'out',\n",
       " 'task_dataset_root_dir': '/home/pablo/nlp-course/assignment/notebooks/../data/pan23/transformed',\n",
       " 'pretrain_dataset_root_dir': '/home/pablo/nlp-course/assignment/notebooks/../data/blogposts',\n",
       " 'device': 'cuda:2'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(root_path, \"configs\", \"base-config.json\")) as f:\n",
    "    config = json.load(f)\n",
    "model_params = config[\"model_params\"]\n",
    "config[\"task_dataset_root_dir\"] = os.path.join(root_path, config[\"task_dataset_root_dir\"])\n",
    "config[\"pretrain_dataset_root_dir\"] = os.path.join(root_path, config[\"pretrain_dataset_root_dir\"])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = config.get(\"device\") if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy version\n",
    "\n",
    "In this version, we will assume that our model will only have to encode one text, and that after encoding the embeddings will be compared through a similarity metric. This is necessary for contrastive pretraining, but not for the supervised part. However, since convolutional layers act locally, they do not provide a comparison between both texts. Thus, we would need to add something to compare separate convolutional embeddings for both texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransformer(nn.Module):\n",
    "    def __init__(self, conv_layers_params: dict, transformer_model: str):\n",
    "        super(ConvTransformer, self).__init__()\n",
    "        self.transformer_model = AutoModel.from_pretrained(transformer_model)\n",
    "        pretrained_embeddings = self.transformer_model.embeddings.word_embeddings.weight.detach().numpy()\n",
    "        num_embeddings, transformed_embedding_dim = pretrained_embeddings.shape\n",
    "        padding_idx = self.transformer_model.embeddings.word_embeddings.padding_idx\n",
    "\n",
    "        self.conv_model = ConvModel(num_embeddings, padding_idx, conv_layers_params)\n",
    "\n",
    "        # initialize conv model embeddings with pretrained embeddings through PCA\n",
    "        conv_embedding_dim = self.conv_model.conv_layers[0].conv.in_channels\n",
    "        pca = PCA(n_components=conv_embedding_dim)\n",
    "        conv_init_embedding = pca.fit_transform(pretrained_embeddings)\n",
    "        conv_init_embedding[padding_idx] = 0.\n",
    "\n",
    "        self.conv_model.embeddings.weight.data = torch.tensor(conv_init_embedding)\n",
    "\n",
    "        # store embedding dimension\n",
    "        self.output_embedding_dim = transformed_embedding_dim + self.conv_model.conv_layers[-1].conv.out_channels\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x_transformed = self.transformer_model(input_ids, attention_mask=attention_mask).pooler_output\n",
    "        x_conv = self.conv_model(input_ids)\n",
    "        return torch.cat((x_transformed, x_conv), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "conv_transformer = ConvTransformer(model_params[\"conv_layers_params\"], model_params[\"transformer_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 896])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(0, 1000, (16, 128))\n",
    "attention_mask = torch.randint(0, 2, (16, 128))\n",
    "conv_transformer(input_ids, attention_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_transformer.output_embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_projection_head(model, input_dim, ff_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        model,\n",
    "        nn.Linear(input_dim, ff_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(ff_dim, output_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTXent loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntxent_loss(emb_1, emb_2, temperature):\n",
    "    device = emb_1.device\n",
    "    batch_size, _ = emb_1.shape\n",
    "\n",
    "    norm_emb_1, norm_emb_2 = F.normalize(emb_1), F.normalize(emb_2)\n",
    "    cos_sim = torch.einsum(\"ax,bx->ab\", norm_emb_1, norm_emb_2)\n",
    "    scaled_cos_sim = cos_sim / temperature\n",
    "\n",
    "    labels = torch.arange(batch_size).to(device)\n",
    "    return 0.5 * F.cross_entropy(scaled_cos_sim, labels) + 0.5 * F.cross_entropy(scaled_cos_sim.T, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1, emb_2 = torch.randn(16, 128), torch.randn(16, 128)\n",
    "temperature = 0.07\n",
    "ntxent_loss(emb_1, emb_2, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_with_projection_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_params[\"transformer_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'test_set_ratio': 0.1,\n",
       " 'epochs': 20,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.01,\n",
       " 'unfrozen_layers': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_params = config[\"pretrain_params\"]\n",
    "pretrain_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's freeze all layers, and defreeze the ones we want to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in conv_transformer.transformer_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "layers = conv_transformer.transformer_model.encoder.layer\n",
    "frozen_layers = len(layers) - pretrain_params[\"unfrozen_layers\"]\n",
    "for layer in layers[frozen_layers:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(conv_transformer.parameters(), lr=pretrain_params[\"lr\"], weight_decay=pretrain_params[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BlogDataset(config[\"pretrain_dataset_root_dir\"])\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))],\n",
    "    generator=torch.Generator(device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=pretrain_params[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=BlogCollatorFn(tokenizer, config[\"max_len\"]),\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=pretrain_params[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=BlogCollatorFn(tokenizer, config[\"max_len\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    losses = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        emb = conv_transformer(input_ids, attention_mask)\n",
    "        emb_1, emb_2 = emb[::2], emb[1::2]\n",
    "        loss = ntxent_loss(emb_1, emb_2, 0.07)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            emb = conv_transformer(input_ids, attention_mask)\n",
    "            emb_1, emb_2 = emb[::2], emb[1::2]\n",
    "            loss = ntxent_loss(emb_1, emb_2, 0.07)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss}, Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With ContrastivePretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.conv_transformer_model import ConvTransformer\n",
    "from src.heads.projection_head import ModelWithProjectionHead\n",
    "from src.trainers.contrastive_pretrainer import ContrastivePretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ConvTransformer(model_params[\"conv_layers_params\"], model_params[\"transformer_model\"])\n",
    "model_with_proj_head = ModelWithProjectionHead(\n",
    "    model,\n",
    "    model.output_embedding_dim,\n",
    "    **model_params[\"projection_head_params\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_params[\"transformer_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'test_set_ratio': 0.1,\n",
       " 'steps': 20000,\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.01,\n",
       " 'unfrozen_layers': 2,\n",
       " 'collator_fn': <src.data_loaders.blogposts.BlogCollatorFn at 0x7fd892cf4550>,\n",
       " 'checkpoint_file': 'conv_transformer_pretrained.pt',\n",
       " 'device': 'cuda:2'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_pretrainer_config = {\n",
    "    **config[\"pretrain_params\"],\n",
    "    \"collator_fn\": BlogCollatorFn(tokenizer, config[\"max_len\"]),\n",
    "    \"checkpoint_file\": \"conv_transformer_pretrained.pt\",\n",
    "    \"device\": device,\n",
    "}\n",
    "contrastive_pretrainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.transformer_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "layers = model.transformer_model.encoder.layer\n",
    "frozen_layers = len(layers) - contrastive_pretrainer_config[\"unfrozen_layers\"]\n",
    "for layer in layers[frozen_layers:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BlogDataset(config[\"pretrain_dataset_root_dir\"])\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainer = ContrastivePretrainer(contrastive_pretrainer_config, model_with_proj_head, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN - iter_num=100, mean_training_loss=3.3908960700035093, mean_eval_loss=2.5963680045358064, (1.22s)\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_721625/1797640451.py\", line 1, in <module>\n",
      "    pretrainer.run()\n",
      "  File \"/home/pablo/nlp-course/assignment/notebooks/../src/trainers/contrastive_pretrainer.py\", line 106, in run\n",
      "    iter_time = tnow\n",
      "  File \"/home/pablo/nlp-course/assignment/notebooks/../src/trainers/contrastive_pretrainer.py\", line 148, in __mean_eval_loss\n",
      "  File \"/home/pablo/nlp-course/assignment/notebooks/../src/losses/ntxent.py\", line 11, in ntxent_loss\n",
      "    labels = torch.arange(batch_size).to(device)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "pretrainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
