{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/master-nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "root_path = os.path.join(os.getcwd(), \"..\") # WARNING: might need to change\n",
    "src_path = os.path.join(root_path, \"src\")\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from models.conv_transformer_model import ConvTransformer\n",
    "from heads.classification_head import ModelWithClassificationHead\n",
    "from trainers.classification_trainer import ClassificationTrainer\n",
    "from data_loaders.pan23 import PAN23Dataset, PAN23CollatorFn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_params': {'conv_layers_params': [{'conv_params': {'in_channels': 32,\n",
       "     'out_channels': 128,\n",
       "     'kernel_size': 5,\n",
       "     'padding': 'same'},\n",
       "    'dim_feedforward': 128,\n",
       "    'dropout_params': {'p': 0.1}}],\n",
       "  'transformer_model': 'roberta-base',\n",
       "  'projection_head_params': {'dropout_p': 0.1,\n",
       "   'ff_dim': 256,\n",
       "   'output_dim': 128},\n",
       "  'classification_head_params': {'dropout_p': 0.1, 'ff_dim': 256}},\n",
       " 'max_len': 512,\n",
       " 'pretrain_params': {'batch_size': 64,\n",
       "  'test_set_ratio': 0.1,\n",
       "  'steps': 20000,\n",
       "  'learning_rate': 0.0001,\n",
       "  'unfrozen_layers': 2},\n",
       " 'pan_train_params': {'batch_size': 16,\n",
       "  'steps': 10000,\n",
       "  'lr': 0.0001,\n",
       "  'unfrozen_layers': 2},\n",
       " 'prefix_file_name': 'conv_transformer_base',\n",
       " 'out_dir': 'out',\n",
       " 'task_dataset_root_dir': '/home/pablo/nlp-course/assignment/notebooks/../data/pan23/transformed',\n",
       " 'pretrain_dataset_root_dir': '/home/pablo/nlp-course/assignment/notebooks/../data/blogposts',\n",
       " 'device': 'cuda:2'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(root_path, \"configs\", \"base-config.json\")) as f:\n",
    "    config = json.load(f)\n",
    "model_params = config[\"model_params\"]\n",
    "config[\"task_dataset_root_dir\"] = os.path.join(root_path, config[\"task_dataset_root_dir\"])\n",
    "config[\"pretrain_dataset_root_dir\"] = os.path.join(root_path, config[\"pretrain_dataset_root_dir\"])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = config.get(\"device\") if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ConvTransformer(model_params[\"conv_layers_params\"], model_params[\"transformer_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_classification_head = ModelWithClassificationHead(\n",
    "    model=model, \n",
    "    input_dim=model.output_embedding_dim,\n",
    "    **model_params[\"classification_head_params\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for task in range(1, 4):\n",
    "    train_datasets.append(\n",
    "        PAN23Dataset(os.path.join(config[\"task_dataset_root_dir\"], f\"pan23-task{task}-train\"))\n",
    "    )\n",
    "    test_datasets.append(\n",
    "        PAN23Dataset(os.path.join(config[\"task_dataset_root_dir\"], f\"pan23-task{task}-validation\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.ConcatDataset(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60233, [2828, 7042, 4112])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), [len(test_dataset) for test_dataset in test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_params[\"transformer_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'steps': 10000,\n",
       " 'lr': 0.0001,\n",
       " 'unfrozen_layers': 2,\n",
       " 'collator_fn': <data_loaders.pan23.PAN23CollatorFn at 0x7f246ce90340>,\n",
       " 'checkpoint_file': 'conv_transformer_pretrained.pt',\n",
       " 'device': 'cuda:2'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_config = {\n",
    "    **config[\"pan_train_params\"],\n",
    "    \"collator_fn\": PAN23CollatorFn(tokenizer, config[\"max_len\"]),\n",
    "    \"checkpoint_file\": \"conv_transformer_pretrained.pt\",\n",
    "    \"device\": device,\n",
    "}\n",
    "trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(transformer_model, num_unfrozen_layers):\n",
    "    for param in transformer_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    layers = transformer_model.encoder.layer\n",
    "    frozen_layers = len(layers) - num_unfrozen_layers\n",
    "    for layer in layers[frozen_layers:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layers(model.transformer_model, trainer_config[\"unfrozen_layers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ClassificationTrainer(trainer_config, model_with_classification_head, train_dataset, test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([0, 1, 0, 0, 0, 0])\n",
    "y_pred = np.array([0, 1, 0, 1, 1, 0])\n",
    "\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN, FP, FN, TP = matrix.ravel()\n",
    "    \n",
    "# Calculating Precision and Recall\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "\n",
    "# Calculating F1 Score\n",
    "F1_Score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.ravel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
